\cleardoublepage
%\pagebreak
\phantomsection
\begin{appendices}
	\chapter{}
	\section{Proof of correctness for BFS}\label{appendix:bfs:correctness}
		We'll prove the correctness of BFS using mathematical induction.
		\begin{itemize}
			\item \textit{Inductive hypothesis}: For all nodes at distance
			$k$ from the source, BFS correctly computes $distance[v] = k$.
			\item \textit{Base case}: The source node $s$ has $distance[s] = 0$.
			\item \textit{Induction step}: Assume the hypothesis is true for nodes at a distance $k$ from $s$. Then their neighbours (nodes at distance $k + 1$) are enqueued and assigned $distance = k + 1$ before any nodes at $distance > k + 1$ are processed.
			\item \textit{Conclusion}: BFS computes the shortest possible path for all reachable nodes.
		\end{itemize}
	\section{Proof of complexity for BFS}\label{appendix:bfs:complexity}
		Let us assume a graph $G(V, E)$ with $V$ vertices and $E$ edges.
	\begin{itemize}
		\item Mark all $V$ vertices as unvisited. This takes $O(V)$ time.
		\item Each vertex enters the queue once (when discovered) and exits the queue once. Enqueue and dequeue operations are $O(1)$, so processing all vertices takes $O(V)$ time.
		\item For each dequeued vertex $u$, iterate through its adjacency list to check all edges $(u,v)$.
		\item In a directed graph, each edge $(u,v)$ is processed once. In an undirected graph, each edge $(u,v)$ is stored twice (once for $u$ and once for $v$), but each is still processed once during BFS.
		\item Summing over all vertices, the total edge-processing time is $O(E)$.
	\end{itemize}
		Thus, the overall time complexity is $O(V+E)$.
	\section{Proof of correctness for Bellman-Ford}\label{appendix:bellford:correctness}
	We'll prove the correctness of Bellman-Ford algorithm using mathematical induction.
	\begin{itemize}
		\item \textit{Inductive hypothesis}: After $k$ iterations, $distance[v]$ is the length of the shortest path from $s$ to $v$ using at most $k$ edges.
		\item \textit{Base case}: After 0 iterations, $distance[s]=0$ (correct), and $distance[v] = \infty$ for all $v \neq s$ (no paths have been explored yet).
		\item \textit{Induction step}: Consider the $(k+1)^{th}$ iteration. For each edge $(u,v)$, if $distance[u]+w(u,v)<distance[v]$, then $distance[v]$ is updated to $distance[u]+w(u,v)$. This ensures that after $k+1$ iterations, $distance[v]$ is the length of the shortest path using at most $k+1$ edges.
		\item \textit{Conclusion}: After $V-1$ iterations, all shortest paths with at most $V-1$ edges have been found. Since a shortest path in a graph with $V$ vertices cannot have more than $V-1$ edges, the algorithm is correct.
		\item \textit{Negative cycle detection}: After $V - 1$ iterations, if any $distance[v]$ can still be improved (i.e. $distance[u]+w(u,v)<distance[v]$ for some edge $(u,v)$), then the graph contains a negative-weight cycle reachable from $s$.
	\end{itemize}
	\section{Proof of complexity for Bellman-Ford}\label{appendix:bellford:complexity}
	Let us assume a graph $G(V, E)$ with $V$ vertices and $E$ edges.
	\begin{itemize}
		\item Set $distance[s]=0$ and $distance[v]=\infty$ for all $v \neq s$. This takes $O(V)$ time.
		\item Relax all $E$ edges, repeated $V - 1$ times. Each relaxation takes $O(1)$ time. This takes a total time of $O(V \cdot E)$.
		\item For negative cycle detection, relax all the edges once more. This takes $O(E)$ time.
	\end{itemize}
	The dominant term is the relaxation step, which takes $O(V \cdot E)$ time, hence the overall complexity of the algorithm is $O(V \cdot E)$.
	\section{Proof of correctness for Dijkstra's}\label{appendix:dijkstra:correctness}
	We'll prove the correctness of Dijkstra's algorithm using mathematical induction.
	\begin{itemize}
		\item \textit{Inductive hypothesis}: After $k$ vertices are extracted from $Q$, their distance distance values are the correct shortest path distances from $s$.
		\item \textit{Base case}: Initially, $distance[s]=0$ (correct), and $distance[v]=\infty$ for all $v \neq s$ (no paths have been explored yet).
		\item \textit{Induction step}: Let $u$ be the $(k+1)^{th}$ vertex extracted from $Q$. Suppose there exists a shorter path to $u$ not using the extracted vertices. This path must leave the set of extracted vertices at some edge $(x,y)$, but since $w(x,y) \geq 0$, this would imply $distance[y]<distance[u]$, contradicting $u$â€™s extraction.
		\item \textit{Conclusion}: After all vertices are processed, the $distance$ array contains the correct shortest path distances.
	\end{itemize}
	\section{Proof of complexity for Dijkstra's}\label{appendix:dijkstra:complexity}
	Let us assume a graph $G(V, E)$ with $V$ vertices and $E$ edges. In a priority-queue based implementation of the algorithm,
	\begin{itemize}
		\item Each vertex is extracted once ($V \times \mbox{Extract-Min})$ and each edge is relaxed once $(E \times \mbox{Decrease-Key})$.
		\item Extract-Min and Decrease-Key take $O(\log{V})$ time in a binary heap.
		\item Extract-Min and Decrease-Key take $O(\log{V})$ and $O(1)$ time respectively in a fibonacci heap.
		\item For a binary heap, $V \times \mbox{Extract-Min}$ takes $O(V\log{V})$ time and $E \times \mbox{Decrease-Key}$ takes $O(E\log{V})$ time $\to$ a total complexity of $O((V + E)\log{V})$
		\item For a fibonacci heap, $V \times \mbox{Extract-Min}$ takes $O(V\log{V})$ time and $E \times \mbox{Decrease-Key}$ takes $O(E)$ time $\to$ a total complexity of $O(V\log{V} + E)$.
	\end{itemize}
\section{Proof of correctness for A* search}\label{appendix:astar:correctness}
	We'll prove the correctness of A* search algorithm using mathematical induction. Let us define the following: \\
	$f(s)$: Estimated total cost of the path from the start node to the goal node, passing through the current node. \\
	$g(s)$: Cost of the shortest path from the start node to the current node. \\
	$h(s)$: Heuristic estimate of the cost from the current node to the goal node.
	\begin{itemize}
		\item \textit{Inductive hypothesis}: At each step, the node $u$ with the smallest $f(u)$ is the one with the smallest estimated total cost to the goal.
		\item \textit{Base case}: Initially, $g(s)=0$ and $f(s)=h(s)$. The start node $s$ is correctly prioritized.
		\item \textit{Induction step}: 
			\begin{itemize}
				\item When $u$ is extracted, its $g(u)$ is the true shortest path cost from $s$ to $u$ (due to admissibility and consistency).
				\item For each neighbor $v$, $f(v)=g(v)+h(v)$ is updated to reflect the best-known path to $v$.
				\item The algorithm continues to explore nodes in order of increasing $f(v)$, ensuring the shortest path is found.
			\end{itemize}
		\item \textit{Conclusion}: If the goal $t$ is reached, $g(t)$ is the true shortest path cost and If $Q$ becomes empty, no path exists.
	\end{itemize}
\section{Proof of correctness for Bidirectional Search}\label{appendix:Bidirectional:correctness}
\section*{Statement}
If a path exists from the start node $s$ to the goal node $t$, bidirectional search will correctly find the shortest such path.

\section*{Assumptions}
\begin{itemize}
	\item The graph is undirected (or directed with properly reversed edges for the backward search).
	\item The graph is unweighted or uniformly weighted, ensuring BFS finds shortest paths.
	\item The searches are synchronized (e.g., alternate expansions between directions).
\end{itemize}

\section*{Proof}

\subsection*{Existence of a Meeting Node}
Let the shortest path length from $s$ to $t$ be $L$. A midpoint node $m$ exists on this path such that:
\begin{itemize}
	\item If $L$ is even, $m$ is at distance $L/2$ from both $s$ and $t$.
	\item If $L$ is odd, $m$ is at distance $\lfloor L/2 \rfloor$ from $s$ and $\lceil L/2 \rceil$ from $t$ (or vice versa).
\end{itemize}

In both cases, the forward search (from $s$) and backward search (from $t$) will reach $m$ after $d$ and $e$ steps, respectively, where $d + e = L$. Thus, $m$ will eventually be included in both frontiers.

\subsection*{Termination}
Since the graph is finite and BFS explores nodes level-by-level, both searches will expand their frontiers incrementally. The algorithm terminates when a common node is found in both frontiers, which is guaranteed due to the existence of $m$.

\subsection*{Optimality of the Path Found}
Suppose the algorithm terminates with a path of length $L' > L$. Let $v$ be the meeting node, so $d_f(v) + d_b(v) = L'$. However, the shortest path implies the existence of a node $u$ where $d_f(u) + d_b(u) = L < L'$. Since BFS explores nodes in order of increasing distance, $u$ would have been encountered in both frontiers when the forward and backward searches reached depths $d_f(u)$ and $d_b(u)$, respectively. This contradicts the assumption that $L' > L$, proving the first meeting node corresponds to the shortest path.

\subsection*{Completeness}
If a path exists, both BFS processes will explore all nodes up to depth $\lceil L/2 \rceil$ (forward) and $\lfloor L/2 \rfloor$ (backward), ensuring the meeting node $m$ is found.

\section{Proof of complexity for Bidirectional Search}\label{appendix:Bidirectional:complexity}
\section*{1. Time Complexity}
\subsection*{Standard BFS}
Explores $O(b^d)$ nodes, as it traverses all nodes up to depth $d$.

\subsection*{Bidirectional Search}
\begin{itemize}
	\item Each search (forward and backward) explores up to depth $\frac{d}{2}$.
	\item Nodes explored by each search: $O(b^{d/2})$.
	\item Total nodes explored: $O(b^{d/2} + b^{d/2}) = O(b^{d/2})$.
\end{itemize}
\textbf{Result:} Time complexity is $O(b^{d/2})$, exponentially better than BFS.

\section*{2. Space Complexity}
\subsection*{Standard BFS}
Stores all nodes at the deepest level, requiring $O(b^d)$ space.

\subsection*{Bidirectional Search}
\begin{itemize}
	\item Each search stores nodes up to depth $\frac{d}{2}$.
	\item Space for each search: $O(b^{d/2})$.
	\item Total space: $O(b^{d/2} + b^{d/2}) = O(b^{d/2})$.
\end{itemize}
\textbf{Result:} Space complexity is $O(b^{d/2})$, matching the time complexity.

\section*{Proof Intuition}
\subsection*{Meeting in the Middle}
The shortest path of length $d$ implies the forward and backward searches meet at depth $\frac{d}{2}$.  
Even if the path length is odd ($d = 2k + 1$), one search reaches depth $k + 1$, but asymptotically, $O(b^{d/2})$ dominates.

\subsection*{Geometric Series Reduction}
\begin{itemize}
	\item Nodes at depth $k$: $b^k$.
	\item Total nodes explored by one direction: $\sum_{k=1}^{d/2} b^k = O(b^{d/2})$.
\end{itemize}

\subsection*{Intersection Check}
Efficiently checking for intersection (e.g., hash tables) ensures the $O(b^{d/2})$ bound holds.

\subsection{Caveats}
\begin{itemize}
	\item Graph Structure: The analysis assumes a tree with uniform branching Cycles or irregular branching may affect practical performance but not the asymptotic bound.
	\item Optimality: Bidirectional BFS guarantees the shortest path only if both searches use BFS.
\end{itemize}

\subsection*{Conclusion}
Bidirectional search reduces the exponent in the complexity by half compared to standard BFS:
\begin{itemize}
	\item \textbf{Time:} $O(b^{d/2})$ vs. $O(b^d)$.
	\item \textbf{Space:} $O(b^{d/2})$ vs. $O(b^d)$.
\end{itemize}
\section{Proof of correctness for Contraction Hierarchies}\label{appendix:contraction:correctness}
	\section*{1. Preprocessing Correctness}
	\textbf{Claim:} The contraction process preserves all shortest paths in the original graph \(G\).
	
	\subsection*{Proof by Induction}
	\textbf{Base Case:}  
	The original graph \(G\) trivially preserves all shortest paths because no nodes have been contracted.
	
	\textbf{Inductive Hypothesis:}  
	Assume that after contracting the first \(k\) nodes, the remaining graph still preserves all shortest paths.
	
	\textbf{Inductive Step:}  
	When contracting the \((k+1)\)-th node \(v\), we check each pair of neighbors \((u, w)\):
	
	\begin{itemize}
		\item \textbf{Case 1:} If the shortest path between \(u\) and \(w\) goes through \(v\), add a shortcut from \(u\) to \(w\) with weight  
		\[
		w(u, w) = w(u, v) + w(v, w).
		\]
		This ensures that paths going through \(v\) are preserved.
		\item \textbf{Case 2:} If there already exists a direct or alternative path between \(u\) and \(w\) not involving \(v\), then the shortcut is redundant but does no harm.
	\end{itemize}
	
	\textbf{Conclusion:} By the inductive hypothesis, after each contraction, the graph still preserves all shortest paths. Therefore, preprocessing maintains correctness.
	
	\section*{2. Query Correctness}
	\textbf{Claim:} The bidirectional Dijkstra search restricted to upward edges computes the shortest path.
	
	\subsection*{Definitions}
	\begin{itemize}
		\item \textbf{Node Ranking:} Each node is assigned a unique rank based on the order of contractions. Let \(\mathrm{rank}(v)\) denote the importance of node \(v\).
		\item \textbf{Upward Edges:} An edge \((u, v)\) is upward if \(\mathrm{rank}(v) > \mathrm{rank}(u)\).
	\end{itemize}
	
	\subsection*{Proof}
	Let \(P = s \to \cdots \to t\) be a shortest path in the original graph, where \(v\) is the highest-ranked node on this path.
	
	\textbf{Forward Search:}  
	Starting from \(s\), the forward search uses only upward edges, ensuring that it will reach the highest-ranked node \(v\).
	
	\textbf{Reverse Search:}  
	Starting from \(t\), the reverse search (using upward edges in the reversed graph) will also reach \(v\) when viewed in reverse.
	
	\textbf{Meeting Point at \(v\):}  
	Both searches meet at \(v\), combining the shortest path from \(s\) to \(v\) and from \(t\) to \(v\). The minimum distance over all meeting points is the shortest path between \(s\) and \(t\).
	
	\subsection*{Formalization}
	\begin{itemize}
		\item Any shortest path can be partitioned at the highest-ranked node \(v\) into two upward subpaths.
		\item The bidirectional search explores all candidates for the highest-ranked node \(v\), ensuring that the shortest path is found.
	\end{itemize}
	
	\section*{Conclusion}
	The correctness of the CH algorithm follows from:
	\begin{itemize}
		\item \textbf{Preprocessing Correctness:} Shortcutting preserves all shortest paths during contractions.
		\item \textbf{Query Correctness:} Bidirectional search efficiently finds the shortest path using the hierarchy structure.
	\end{itemize}
	
	This proof demonstrates that the CH algorithm guarantees correct results while reducing the search space through node contractions and hierarchical search.


\section{Proof of complexity for Contraction Hierarchies}\label{appendix:constraction:complexity}
	\section*{1. Preprocessing Complexity}
	The preprocessing step involves contracting nodes and adding shortcuts. Let's break down the steps:
	
	\subsection*{Step 1: Ranking the Nodes}
	\begin{itemize}
		\item Each node in the graph is assigned a rank based on its importance.
		\item Ranking is done by contracting nodes one by one and observing the number of shortcuts required.
		\item \textbf{Complexity:} \(O(n \log n)\) where \(n\) is the number of nodes.
	\end{itemize}
	
	\subsection*{Step 2: Contracting Nodes and Adding Shortcuts}
	\begin{itemize}
		\item For each node, we analyze its neighbors to decide if shortcuts are needed.
		\item Let \(d\) be the average degree of a node (the number of connections).
		\item \textbf{Complexity:} \(O(m + n \cdot d^2)\), where \(m\) is the number of edges.
	\end{itemize}
	
	\subsection*{Overall Preprocessing Complexity}
	Combining the complexity for ranking and contraction, the overall preprocessing complexity becomes:
	\[
	O(n \cdot d^2 + m)
	\]
	
	\section*{2. Query Complexity}
	The query step uses a bidirectional Dijkstra search restricted to upward edges.
	
	\subsection*{Step 1: Forward and Reverse Search}
	\begin{itemize}
		\item The search proceeds from the source and target nodes simultaneously, using only upward edges.
	\end{itemize}
	
	\subsection*{Step 2: Meeting at the Highest-Ranked Node}
	\begin{itemize}
		\item The two searches meet at the highest-ranked node on the shortest path.
		\item The search space is reduced because we only consider upward edges.
	\end{itemize}
	
	\subsection*{Search Complexity}
	Let \(h\) be the height of the hierarchy, which is typically much smaller than the number of nodes. The search complexity becomes:
	\[
	O(h \log h)
	\]
	
	\section*{Summary}
	\begin{itemize}
		\item \textbf{Preprocessing Complexity:} \(O(n \cdot d^2 + m)\)
		\item \textbf{Query Complexity:} \(O(h \log h)\)
	\end{itemize}
	
	The preprocessing may be computationally expensive for large graphs, but the query step becomes highly efficient, making CH well-suited for applications requiring fast real-time path queries.

\section{Proof of correctness for ALT}\label{appendix:ALT:correctness}

\begin{itemize}
	\item Admissibility of $h(v)$: A heuristic is admissible if it never overestimates the true distance:  
	\begin{equation*} h(v) \leq d(v, t) \end{equation*}
	By the \textit{triangle inequality}: \begin{equation*} d(v, t) \geq |d(L, v) - d(L, t)| \quad \forall L \in L \end{equation*}
	Taking the maximum over all landmarks:
	\begin{equation*}
		d(v, t) \geq \max_{L \in \textit{L}} \left| d(L, v) - d(L, t) \right| = h(v)
	\end{equation*}
	Thus, $ h(v) $ is admissible.
	\item Consistency of $h(v)$: A heuristic is consistent if for any edge $ (u, v) $:
	\begin{equation*}
		h(v) \leq d(u, v) + h(u)
	\end{equation*}
	Using the \textit{triangle inequality}:
	\begin{equation*} 
		|d(L, v) - d(L, t)| \leq d(u, v) + |d(L, u) - d(L, t)| 
	\end{equation*}
	Taking the maximum over all landmarks:
	\begin{equation*}
		h(v) = \max_{L \in \textit{L}} \left| d(L, v) - d(L, t) \right| \leq d(u, v) + h(u)
	\end{equation*}
	Thus, $ h(v) $ is consistent.
\end{itemize}
Since $ h(v) $ is both admissible and consistent, the ALT algorithm guarantees optimal shortest paths.

\section{Proof of complexity for ALT}\label{appendix:ALT:complexity}
	\begin{itemize}
		\item Landmark Selection:
			\begin{itemize}
				\item \textit{Randomly selecting landmarks}: This is a constant-time operation, $ O(1) $.
				\item \textit{Selecting high-degree or far-apart nodes}: This might involve sorting the nodes based on degree or distance, which would take $ O(|V| \log |V|) $, where $ |V| $ is the number of nodes in the graph.
			\end{itemize}
		\item Precompute Shortest Path Distances from Landmarks: For $k$ landmarks, we need to perform Dijkstraâ€™s algorithm $k$ times, one for each landmark, making the total preprocessing time complexity is $O(k \cdot (|V| + |E|) \log |V|)$.
		\item Query Phase Complexity: The A* search algorithm with ALT uses the ALT heuristic $ h(v) $ instead of a simple heuristic (like Euclidean distance). The time complexity of the A* search depends on the number of nodes expanded during the search and the priority queue operations. In the worst case, the complexity is $ O((|V| + |E|) \log |V|) $. The average query time is generally much faster, but it is difficult to bound precisely without empirical data.		
	\end{itemize}

\section{Proof of correctness for Hub Labellling}\label{appendix:Hub Labelling:correctness}

	The Hub Labeling algorithm assigns a label $ L(v) $ to each node $ v $, which contains hub-distance pairs $ (h, d(v, h)) $ , where $ h $ is a hub node and $ d(v, h) $ is the distance from node $ v $ to hub $ h $. The shortest path $ d(u, v) $ between nodes $ u $ and $ v $ is computed as:

	\begin{equation}
		d(u, v) = \min_{h \in \textit{L}(u) \cap \textit{L}(v)} \left( d(u, h) + d(h, v) \right)
	\end{equation}
	
	where $ L(u) $ and $ L(v) $ are the labels for nodes $ u $ and $ v $, respectively and $ h $ is a common hub in both labels $ L(u) $ and $ L(v) $.
	
	\subsubsection{Label Cover Property}
	
	During the preprocessing phase of the Hub Labeling algorithm, we ensure that for any pair of nodes $ u $ and $ v $, their labels $ L(u) $ and $ L(v) $ share at least one common hub $ h $ that lies on the shortest path between $ u $ and $ v $. This property is critical because it guarantees that the shortest path can always be reconstructed using the shared hub. \medskip
	
	Formally, for any two nodes $ u, v \in V $, there exists a hub $ h \in L(u) \cap L(v) $ such that $ h $ lies on the shortest path between $ u $ and $ v $, i.e., the path $ u \to h \to v $ is a valid shortest path. \medskip
	
	Thus, this \textit{Label Cover Property} ensures that the correct hubs are selected in the query phase and the algorithm can always compute the shortest path.
	
	\subsubsection{Correctness of the Query}
	
	For any two nodes $ u $ and $ v $, during the query phase, the algorithm scans their labels $ L(u) $ and  $ L(v) $ to find the hub $ h $ that minimizes the expression $ d(u, h) + d(h, v) $. This is equivalent to finding the shortest path between $ u $  and $ v $ by traversing through a common hub $ h $. \medskip
	
	Since $ h $ lies on the shortest path between $ u $ and $ v $ (by the Label Cover Property), the value $ d(u, h) + d(h, v) $ is guaranteed to be the shortest path distance between $ u $ and $ v $. \medskip
	
	Therefore, the Hub Labeling algorithm correctly computes the shortest path for any query $ (u, v) $, as it always finds the optimal hub $ h $ and ensures that the sum of distances $ d(u, h) + d(h, v) $ corresponds to the actual shortest path distance.
	
	Thus, the \textit{Hub Labeling Algorithm} is correct.
	

%-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------



\section{Proof of complexity for Hub Labelling}\label{appendix:Hub Labelling:Complexity}

	\subsubsection{Preprocessing Complexity}
	
	The preprocessing phase involves computing the labels for all nodes. This is often done using \textit{hierarchical decomposition} or \textit{Contraction Hierarchies (CH)}, which are techniques used to optimize the process by simplifying the graph and reducing the number of nodes that need to be considered.
	
	
	Let $ |V| $ be the number of vertices in the graph, and let $|E| $ be the number of edges.
	
	\begin{itemize}

	\item \textit{Worst-case Complexity}: 
	The worst-case complexity for preprocessing depends on the structure of the graph. For general graphs, it can be $ O(|V|^2) $, where every node needs to be processed with respect to all other nodes. 
	
	\item \textit{Road Networks}, which typically have hierarchical properties, the preprocessing complexity is often \textit{subquadratic}. This means that in practice, the preprocessing time may be significantly lower than $ O(|V|^2) $.
	
	\item \textit{Preprocessing complexity} : $ O(|V|^2) $ worst case and \textit{Subquadratic} for Road Networks.
	
		\end{itemize}
	\subsubsection{Query Complexity}
	
	The query time depends on the size of the labels $ |L(u)| $ and $|L(v)| $ for the nodes $ u $ and $ v $ being queried. Specifically, the query involves finding a common hub $ h \in L(u) \cap L(v) $, and the query time is proportional to the size of the intersection of the labels. 
	
	\begin{itemize}
	
	\item \textit{Worst-case Complexity}: The worst-case query time is determined by the minimum size of the two labels $ L(u) $ and $ L(v) $, so the complexity is:
	$
	O(\min(|L(u)|, |L(v)|))
	$
	
	\item\textit{Practical Consideration}: In practice, the size of the labels is often small because they grow logarithmically in the size of the graph. As a result, queries tend to be near constant time, i.e., $ O(1) $ for typical use cases.
	
	\item Thus, the \textit{query complexity} is:
	$
	O(\min(|L(u)|, |L(v)|)) \quad \textit{in the worst case, and typically near constant time in practice.}
	$
	
	\end{itemize}
	
	\subsubsection{Space Complexity}
	
	The space complexity is determined by the storage required for all the labels of the nodes in the graph. Each label consists of a set of hub-distance pairs $ (h, d(v, h)) $, and the size of each label is typically denoted by $|L| $.
	
	\begin{itemize}
	\item	\textit{Space Complexity}: The total space required to store labels for all nodes is proportional to the number of nodes $|V|$ and the average label size $ |L| $. Thus, the space complexity is:	$ O(|V| \cdot |L|) $ 
	
	where:
	
	 $|L| $ is the average label size.
	
	\end{itemize}
	
	

	
	
	
	
%---------------------------------------------------------------------------------------------------------------------------------------------------------------------------
	
\end{appendices}