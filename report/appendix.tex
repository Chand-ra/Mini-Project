\cleardoublepage
%\pagebreak
\phantomsection
\begin{appendices}
	\chapter{}
	\section{Proof of correctness for BFS$^{\cite{BFS-ref-one}}$}\label{appendix:bfs:correctness}
		We'll prove the correctness of BFS using mathematical induction.
		\begin{itemize}
			\item \textit{Inductive hypothesis}: For all nodes at distance
			$k$ from the source, BFS correctly computes $distance[v] = k$.
			\item \textit{Base case}: The source node $s$ has $distance[s] = 0$.
			\item \textit{Induction step}: Assume the hypothesis is true for nodes at a distance $k$ from $s$. Then their neighbours (nodes at distance $k + 1$) are enqueued and assigned $distance = k + 1$ before any nodes at $distance > k + 1$ are processed.
			\item \textit{Conclusion}: BFS computes the shortest possible path for all reachable nodes.
		\end{itemize}
	\section{Proof of complexity for BFS$^{\cite{BFS-ref-one}}$}\label{appendix:bfs:complexity}
		Let us assume a graph $G(V, E)$ with $V$ vertices and $E$ edges.
	\begin{itemize}
		\item Mark all $V$ vertices as unvisited. This takes $O(V)$ time.
		\item Each vertex enters the queue once (when discovered) and exits the queue once. Enqueue and dequeue operations are $O(1)$, so processing all vertices takes $O(V)$ time.
		\item For each dequeued vertex $u$, iterate through its adjacency list to check all edges $(u,v)$.
		\item In a directed graph, each edge $(u,v)$ is processed once. In an undirected graph, each edge $(u,v)$ is stored twice (once for $u$ and once for $v$), but each is still processed once during BFS.
		\item Summing over all vertices, the total edge-processing time is $O(E)$.
	\end{itemize}
		Thus, the overall time complexity is $O(V+E)$.
	\section{Proof of correctness for Bellman-Ford$^{\cite{Bellman-ford-ref-one}}$}\label{appendix:bellford:correctness}
	We'll prove the correctness of Bellman-Ford algorithm using mathematical induction.
	\begin{itemize}
		\item \textit{Inductive hypothesis}: After $k$ iterations, $distance[v]$ is the length of the shortest path from $s$ to $v$ using at most $k$ edges.
		\item \textit{Base case}: After 0 iterations, $distance[s]=0$ (correct), and $distance[v] = \infty$ for all $v \neq s$ (no paths have been explored yet).
		\item \textit{Induction step}: Consider the $(k+1)^{th}$ iteration. For each edge $(u,v)$, if $distance[u]+w(u,v)<distance[v]$, then $distance[v]$ is updated to $distance[u]+w(u,v)$. This ensures that after $k+1$ iterations, $distance[v]$ is the length of the shortest path using at most $k+1$ edges.
		\item \textit{Conclusion}: After $V-1$ iterations, all shortest paths with at most $V-1$ edges have been found. Since a shortest path in a graph with $V$ vertices cannot have more than $V-1$ edges, the algorithm is correct.
		\item \textit{Negative cycle detection}: After $V - 1$ iterations, if any $distance[v]$ can still be improved (i.e. $distance[u]+w(u,v)<distance[v]$ for some edge $(u,v)$), then the graph contains a negative-weight cycle reachable from $s$.
	\end{itemize}
	\section{Proof of complexity for Bellman-Ford$^{\cite{Bellman-ford-ref-one}}$}\label{appendix:bellford:complexity}
	Let us assume a graph $G(V, E)$ with $V$ vertices and $E$ edges.
	\begin{itemize}
		\item Set $distance[s]=0$ and $distance[v]=\infty$ for all $v \neq s$. This takes $O(V)$ time.
		\item Relax all $E$ edges, repeated $V - 1$ times. Each relaxation takes $O(1)$ time. This takes a total time of $O(V \cdot E)$.
		\item For negative cycle detection, relax all the edges once more. This takes $O(E)$ time.
	\end{itemize}
	The dominant term is the relaxation step, which takes $O(V \cdot E)$ time, hence the overall complexity of the algorithm is $O(V \cdot E)$.
	\section{Proof of correctness for Dijkstra's$^{\cite{Dijikstra-ref-one}}$}\label{appendix:dijkstra:correctness}
	We'll prove the correctness of Dijkstra's algorithm using mathematical induction.
	\begin{itemize}
		\item \textit{Inductive hypothesis}: After $k$ vertices are extracted from $Q$, their distance distance values are the correct shortest path distances from $s$.
		\item \textit{Base case}: Initially, $distance[s]=0$ (correct), and $distance[v]=\infty$ for all $v \neq s$ (no paths have been explored yet).
		\item \textit{Induction step}: Let $u$ be the $(k+1)^{th}$ vertex extracted from $Q$. Suppose there exists a shorter path to $u$ not using the extracted vertices. This path must leave the set of extracted vertices at some edge $(x,y)$, but since $w(x,y) \geq 0$, this would imply $distance[y]<distance[u]$, contradicting $u$â€™s extraction.
		\item \textit{Conclusion}: After all vertices are processed, the $distance$ array contains the correct shortest path distances.
	\end{itemize}
	\section{Proof of complexity for Dijkstra's $^{\cite{Dijikstra-ref-one}}$}\label{appendix:dijkstra:complexity}
	Let us assume a graph $G(V, E)$ with $V$ vertices and $E$ edges. In a priority-queue based implementation of the algorithm,
	\begin{itemize}
		\item Each vertex is extracted once ($V \times \mbox{Extract-Min})$ and each edge is relaxed once $(E \times \mbox{Decrease-Key})$.
		\item Extract-Min and Decrease-Key take $O(\log{V})$ time in a binary heap.
		\item Extract-Min and Decrease-Key take $O(\log{V})$ and $O(1)$ time respectively in a fibonacci heap.
		\item For a binary heap, $V \times \mbox{Extract-Min}$ takes $O(V\log{V})$ time and $E \times \mbox{Decrease-Key}$ takes $O(E\log{V})$ time $\to$ a total complexity of $O((V + E)\log{V})$
		\item For a fibonacci heap, $V \times \mbox{Extract-Min}$ takes $O(V\log{V})$ time and $E \times \mbox{Decrease-Key}$ takes $O(E)$ time $\to$ a total complexity of $O(V\log{V} + E)$.
	\end{itemize}
	\section{Proof of correctness for A* search$^{\cite{A-star-Search-ref-one}}$}\label{appendix:astar:correctness}

	We'll prove the correctness of A* search algorithm using mathematical induction. Let us define the following: \\
	$f(s)$: Estimated total cost of the path from the start node to the goal node, passing through the current node. \\
	$g(s)$: Cost of the shortest path from the start node to the current node. \\
	$h(s)$: Heuristic estimate of the cost from the current node to the goal node.
	\begin{itemize}
		\item \textit{Inductive hypothesis}: At each step, the node $u$ with the smallest $f(u)$ is the one with the smallest estimated total cost to the goal.
		\item \textit{Base case}: Initially, $g(s)=0$ and $f(s)=h(s)$. The start node $s$ is correctly prioritized.
		\item \textit{Induction step}: 
			\begin{itemize}
				\item When $u$ is extracted, its $g(u)$ is the true shortest path cost from $s$ to $u$ (due to admissibility and consistency).
				\item For each neighbor $v$, $f(v)=g(v)+h(v)$ is updated to reflect the best-known path to $v$.
				\item The algorithm continues to explore nodes in order of increasing $f(v)$, ensuring the shortest path is found.
			\end{itemize}
		\item \textit{Conclusion}: If the goal $t$ is reached, $g(t)$ is the true shortest path cost and If $Q$ becomes empty, no path exists.
	\end{itemize}
\section{Proof of correctness for Bidirectional Search$^{\cite{Bidirectional-ref-one}}$}\label{appendix:Bidirectional:correctness}
	\begin{itemize}
		\item Let the shortest path length from $s$ to $t$ be $L$. A midpoint node $m$ exists on this path such that:
		\begin{itemize}
			\item If $L$ is even, $m$ is at distance $L/2$ from both $s$ and $t$.
			\item If $L$ is odd, $m$ is at distance $\lfloor L/2 \rfloor$ from $s$ and $\lceil L/2 \rceil$ from $t$ (or vice versa).
		\end{itemize}
		In both cases, the forward search (from $s$) and backward search (from $t$) will reach $m$ after $d$ and $e$ steps, respectively, where $d + e = L$. Thus, $m$ will eventually be included in both frontiers.
		\item Suppose the algorithm terminates with a path of length $L' > L$. Let $v$ be the meeting node, so $d_f(v) + d_b(v) = L'$. However, the shortest path implies the existence of a node $u$ where $d_f(u) + d_b(u) = L < L'$. Since BFS explores nodes in order of increasing distance, $u$ would have been encountered in both frontiers when the forward and backward searches reached depths $d_f(u)$ and $d_b(u)$, respectively. This contradicts the assumption that $L' > L$, proving the first meeting node corresponds to the shortest path.
	\end{itemize}
\section{Proof of complexity for Bidirectional Search$^{\cite{Bidirectional-ref-one}}$}\label{appendix:Bidirectional:complexity}
	\begin{itemize}
		\item The shortest path of length $d$ implies the forward and backward searches meet at depth $\frac{d}{2}$. Even if the path length is odd ($d = 2k + 1$), one search reaches depth $k + 1$, but asymptotically, $O(b^{d/2})$ dominates.
		\item Each search (forward and backward) explores up to depth $\frac{d}{2}$.
		\item  Nodes explored by each search: $O(b^{d/2}) \implies$ total nodes explored: $O(b^{d/2} + b^{d/2}) = O(b^{d/2})$.
		\item Also, each search stores nodes up to depth $\frac{d}{2} \implies$  space for each search: $O(b^{d/2}) \implies$ Total space: $O(b^{d/2} + b^{d/2}) = O(b^{d/2})$.
	\end{itemize}

\section{Proof of correctness for Contraction Hierarchies$^{\cite{Contraction-hiearchies-ref-one}}$}\label{appendix:contraction:correctness}
	 Since the query phase uses Bidirectional Search, the proof of it's correctness can be referred to at \textbf{Appendix \ref{appendix:Bidirectional:correctness}}. We'll prove the correctness of the query phase using mathematical induction.
	\begin{itemize}
		\item \textit{Inductive hypothesis}: Assume that after contracting the first \(k\) nodes, the remaining graph still preserves all shortest paths.
		\item \textit{Base case}: The original graph \(G\) trivially preserves all shortest paths because no nodes have been contracted.
		\item \textit{Induction step}: 	When contracting the $(k+1)^{th}$ node \(v\), we check each pair of neighbors \((u, w)\):
		\begin{itemize}
			\item Case 1: If the shortest path between \(u\) and \(w\) goes through \(v\), add a shortcut from \(u\) to \(w\) with weight  
			\[ w(u, w) = w(u, v) + w(v, w) \]
			This ensures that paths going through \(v\) are preserved.
			\item Case 2: If there already exists a direct or alternative path between \(u\) and \(w\) not involving \(v\), then the shortcut is redundant but does no harm.
		\end{itemize}
		\item \textit{Conclusion}: By the inductive hypothesis, after each contraction, the graph still preserves all shortest paths. Therefore, preprocessing maintains correctness.
	\end{itemize}

\section{Proof of complexity for Contraction Hierarchies$^{\cite{Contraction-hiearchies-ref-one}}$}\label{appendix:constraction:complexity}
	\begin{itemize}
		\item Assign a unique rank (or order) to each node in the graph. This rank determines the order in which nodes are contracted. This takes time $O(V \log V)$, where $V$ is the number of nodes.
		\item For each node $v$ in order of increasing rank:
			\begin{itemize}
				\item Remove $v$ from the graph.
				\item Add shortcuts between pairs of neighbors of $v$ if the shortest path between them passes through $v$.
				\item Store the shortcuts and the original edges in a hierarchical structure.
			\end{itemize}
			This takes time $O(V \cdot d^{2})$, where $d$ is the average degree of a node.
		\item The graph is divided into levels based on node ranks, higher-ranked nodes are at higher levels in the hierarchy. This takes time $O(V + E + S)$ where $S$ is the numbet of shortcuts.
		\item The query phase can be computed using Dijkstra's, which has a time complexity of $O((V + E) \log V)$.
	\end{itemize}

\section{Proof of correctness for ALT$^{\cite{ALT-ref-one}}$}\label{appendix:ALT:correctness}

\begin{itemize}
	\item Admissibility of $h(v)$: A heuristic is admissible if it never overestimates the true distance:  
	\begin{equation*} h(v) \leq d(v, t) \end{equation*}
	By the \textit{triangle inequality}: \begin{equation*} d(v, t) \geq |d(L, v) - d(L, t)| \quad \forall L \in L \end{equation*}
	Taking the maximum over all landmarks:
	\begin{equation*}
		d(v, t) \geq \max_{L \in \textit{L}} \left| d(L, v) - d(L, t) \right| = h(v)
	\end{equation*}
	Thus, $ h(v) $ is admissible.
	\item Consistency of $h(v)$: A heuristic is consistent if for any edge $ (u, v) $:
	\begin{equation*}
		h(v) \leq d(u, v) + h(u)
	\end{equation*}
	Using the \textit{triangle inequality}:
	\begin{equation*} 
		|d(L, v) - d(L, t)| \leq d(u, v) + |d(L, u) - d(L, t)| 
	\end{equation*}
	Taking the maximum over all landmarks:
	\begin{equation*}
		h(v) = \max_{L \in \textit{L}} \left| d(L, v) - d(L, t) \right| \leq d(u, v) + h(u)
	\end{equation*}
	Thus, $ h(v) $ is consistent.
\end{itemize}
Since $ h(v) $ is both admissible and consistent, the ALT algorithm guarantees optimal shortest paths.

\section{Proof of complexity for ALT$^{\cite{ALT-ref-one}}$}\label{appendix:ALT:complexity}
	\begin{itemize}
		\item Landmark Selection:
			\begin{itemize}
				\item \textit{Randomly selecting landmarks}: This is a constant-time operation, $ O(1) $.
				\item \textit{Selecting high-degree or far-apart nodes}: This might involve sorting the nodes based on degree or distance, which would take $ O(|V| \log |V|) $, where $ |V| $ is the number of nodes in the graph.
			\end{itemize}
		\item Precompute Shortest Path Distances from Landmarks: For $k$ landmarks, we need to perform Dijkstraâ€™s algorithm $k$ times, one for each landmark, making the total preprocessing time complexity is $O(k \cdot (|V| + |E|) \log |V|)$.
		\item Query Phase Complexity: The A* search algorithm with ALT uses the ALT heuristic $ h(v) $ instead of a simple heuristic (like Euclidean distance). The time complexity of the A* search depends on the number of nodes expanded during the search and the priority queue operations. In the worst case, the complexity is $ O((|V| + |E|) \log |V|) $. The average query time is generally much faster, but it is difficult to bound precisely without empirical data.		
	\end{itemize}

\section{Proof of correctness for Hub Labellling$^{\cite{HL-ref-one}}$}\label{appendix:Hub Labelling:correctness}
	\begin{itemize}
		\item During the preprocessing phase of the Hub Labeling algorithm, we ensure that for any pair of nodes $ u $ and $ v $, their labels $ L(u) $ and $ L(v) $ share at least one common hub $ h $ that lies on the shortest path between $ u $ and $ v $.
		\item Formally, for any two nodes $ u, v \in V $, there exists a hub $ h \in L(u) \cap L(v) $ such that $ h $ lies on the shortest path between $ u $ and $ v $, i.e., the path $ u \to h \to v $ is a valid shortest path. Thus, this \textit{Label Cover Property} ensures that the correct hubs are selected in the query phase and the algorithm can always compute the shortest path.
		\item For any two nodes $ u $ and $ v $, during the query phase, the algorithm scans their labels $ L(u) $ and  $ L(v) $ to find the hub $ h $ that minimizes the expression $ d(u, h) + d(h, v) $. This is equivalent to finding the shortest path between $ u $  and $ v $ by traversing through a common hub $ h $.
		\item Since $ h $ lies on the shortest path between $ u $ and $ v $ (by the Label Cover Property), the value $ d(u, h) + d(h, v) $ is guaranteed to be the shortest path distance between $ u $ and $ v $.
		\item Therefore, the Hub Labeling algorithm correctly computes the shortest path for any query $ (u, v) $, as it always finds the optimal hub $ h $ and ensures that the sum of distances $ d(u, h) + d(h, v) $ corresponds to the actual shortest path distance.
	\end{itemize}

\section{Proof of complexity for Hub Labelling$^{\cite{HL-ref-one}}$}\label{appendix:Hub Labelling:Complexity}
	\begin{itemize}
		\item For each node $v$, compute the forward and backward labels using Dijkstra's algorithm or a similar shortest path algorithm.
		\item The time complexity for computing labels for all nodes is $O(V \cdot (V+E)\log V)$, where $V$ is the number of nodes and $E$ is the number of edges.
		\item The selection of hubs can be done using various strategies, such as selecting nodes with high centrality or using a greedy algorithm. The time complexity for hub selection is $O(VlogV)$.
		\item For a given query $(s,t)$, the forward label of $s$ and the backward label of $t$ are intersected to find the shortest path distance.
		\item The time complexity for label intersection is $O(|L_f(s)| + |L_b(t)|)$, where $|L_f(s)|$ and $|L_b(t)|$ are the sizes of the forward and backward labels, respectively.
		\item Each node stores a forward label and a backward label. The space complexity for storing all labels is $O(V \cdot L)$, where $L$ is the average label size.
	\end{itemize}
	
\end{appendices}